
---
# T4 - GPUs y CUDA

[Atrás](../README.md)

---
## Arquitectura CPU y GPU
### Modelos de CPU
- **Modelo secuencial / tradicional**
	- Se ejecuta paso a paso.
- **SIMT - Single Instruction, Multiple Threads**
	- Se divide el trabajo en varios hilos (pocos hilos)
- **SIMD - Single Instruction , Multiple Data**
	- Se opera con vectores pequeños en un solo ciclo.
### Modelo de GPU
#### Física
1. **GPU - Graphics Processing Unit**
	- Dispone de una memoria global (VRAM)
2. **GPG - Graphics Processing Cluster**
3. **TPC - Texture Processing Cluster**
4. **SMs - Streaming Multiprocessors**
	- Tiene una memoria compartida más rapida (Shared Memory).
	- Dispone de un warp scheduler.
5. **Cuda Cores**
	- Tiene una memoria más rapida (Register File).
#### Lógica
1. **Device (GPU)**
	- Se refiere a la GPU.
	- Ejecuta los kernels.
2. **Malla de bloques (Grid)**
	- Representa el problema en su totalidad.
	- Es el conjunto de bloques definido al lanzar el kernel.
3. **Bloque de hilos (Thread Block)**
	- Unidad programable más pequeña.
	- Conjunto de hilos (max. 1024) que se define al lanzar un kernel.
	- Pueden trabajar en 1D, 2D y 3D. (Cuidado con el max. hilos)
	- Los bloques se asignan a un SM para que los ejecute.
4. **Warp**
	- Unidad ejecutable más pequeña.
	- Agrupación de 32 hilos gestinados por el Warp Scheduler.
	- Todos sus hilos se ejecutan de manera sincronizada.
	- No todos los hilos de un warp ejecutan instrucciones.
5. **Hilo**
	- El hilo es la unidad más pequeña de ejecución.
	- Cada hilo se ejecutará en un núcleo CUDA

---
## Nomenclatura de Matrices

![](../assets/Nomenclatura_matrices.svg)

---
## CUDA - Kernels
### Definición
- Define el código que se ejecutará en cada hilo.
- Los hilos se agrupan en bloques. Cada hilo tiene:
	- Un ID de bloque.
	- Un ID de hilo dentro del bloque.
- Estos IDs se pueden acceder con variables intrínsecas, pudiendo:
	- Variar el flujo de ejecución (con `if`).
	- Acceder a distintos datos.

```c++
// Definición
__global__ void nombre_kernel(...) { ... }

// Llamada
nombre_kernel<<<numero_bloques, hilos_por_bloque>>>(...)
```

### Ejecución de kernels
- La ejecución es asíncrona con respecto a la CPU:
	- La CPU llama al kernel y sigue ejecutando sin esperar a que termine.
	- Para esperar podemos usar la función: `cudaDeviceSynchronize()`
- Los bloques se ejecutan sin sincronización entre ellos.
	- Cada bloque se asigna a un SM.
	- Los bloques se dividen en warps de 32 hilos.
- Los hilos de un bloque:
	- Pueden sincronizarse con la barrera: `__syncthreads()`
	- Pueden compartir datos con la Shared Memory. Se define como: `__shared__`
- Se pueden definir variables en la GPU con `__device__`
	- Accesible por los hilos de todos los bloques.

### Dimensiones
#### 1D - Dimension X
```c++
#define TPB 3
#define NB(size) (size + TPB - 1) / TPB

// Tenemos un vector V de tamaño size
__global__ void kernel_1D(int *V, int size)
{
	int i = blockIdx.x * blockDim.x + threadIdx.x; // ID de hilo en grid.

	if(i < size) { ... } // Comprobamos que estamos dentro de los márgenes.
}

kernel_1D<<< NB(size) , TPB >>>(V, size);
```

![](../assets/Cuda_1D.svg)
#### 2D - Dimensiones X e Y
```c++
#define TPB_X = 3
#define TPB_Y = 2

#define TPB() dim3(TPB_X, TPB_Y)
#define NB(m, n) dim3( (m + TPB_X - 1) / TPB_X , (n + TPB_Y - 1) / TPB_Y)

// Tenemos una matriz M en column major de dimensiones nxm
__global__ void kernel_2D(int *M, int m, int n)
{
	int j = blockIdx.x * blockDim.x + threadIdx.x; // Columna del hilo en grid
	int i = blockIdx.y * blockDim.y + threadIdx.y; // Fila del hilo en grid

	if (i < m && j < n) { ... } // Comprobamos que estamos dentro de los márgenes.
}

kernel_2D<<< NB(m, n) , TPB() >>>(M, m, n);
```

![](../assets/Cuda_2D.svg)
 
---
## CUDA - Errores
#### Errores en funciones
```C++
// Simple
if (cudaMemcpy(...) != cudaSuccess) {...}
```

```c++
// Macro
#define CUDAERR(x) do { if((x)!=cudaSuccess) { \
	printf("CUDA error: %s : %s, line %d\n", cudaGetErrorString(x), __FILE__, __LINE__);\
	return EXIT_FAILURE;}} while(0)

// Uso
CUDAERR(cudaMemcpy(...));
```
#### Errores en kernels
```C++
// Simple
kernel<<<...>>>(...);
if (cudaSuccess != cudaGetLastError()) {...}
```

```c++
// Macro
#define CUDACHECK() __getLastCudaError(__FILE__, __LINE__)

inline void __getLastCudaError(const char *file, const int line)
{
	if (cudaSuccess != cudaGetLastError())
	{
		fprintf(stderr, "%s(%i): getLastCudaError() error: (%d) %s.\n",
			file, line, (int)err, cudaGetErrorString(err));
		exit(-1);
	}
}

// Uso
kernel<<<...>>>(...);
CUDACHECK();
```
---
## CUDA - Eventos 

- Normalmente usados para medir tiempo de ejecución de kernels.
``` C++
cudaEvent_t INICIO, FIN; // Declaramos los eventos.

cudaEventCreate(&INICIO); // Inicializamos los eventos
cudaEventCreate(&FIN);

cudaEventRecord(INICIO, 0); // Obtenemos el tiempo antes del kernel

kernel<<<...>>>(...); // Lanzamos el kernel

cudaEventRecord(FIN, 0); // Obtenemos el tiempo despues del kernel
cudaEventSynchronize(FIN); // Sincronizamos

float time; // Obtenemos la diferencia
cudaEventElapsedTime(&time, INICIO, FIN);

cudaEventDestroy(INICIO); // Destruimos los eventos
cudaEventDestroy(FIN);
```

---
## CUDA - Memoria y transferencias con CPU
### Jerarquía
1. Cada hilo tiene su memoria local. (Funciona con cachés L1 y L2 al igual que la CPU)
2. Cada bloque tiene una memoria compartida (Shared Memory)
3. La GPU tiene una memoria global (además de otras para texturas y constantes)
### Modelo clásico
- Datos duplicados en CPU y en GPU.
- Hay que reservar espacio en GPU y transferir los datos de la CPU.
```C
// Declaramos variables necesarias.
double *Host_V, *Device_V;

// Creamos un vector. A diferencia de malloc lo inicializa a 0.
Host_V = (double*) calloc(size * sizeof(double));

// Reservamos el vector en memoria de la GPU.
cudaMalloc( (void**) &Device_V, size * sizeof(double))

// Copiamos los datos a la memoria de la GPU
cudaMemcpy(Host_V, Device_V, size * sizeof(double), cudaMemcpyDeviceToHost)

// ...

// Copiamos los resultados de vuelta a la CPU
cudaMemcpy(Device_V, Host_V, size * sizeof(double), cudaMemcpyHostToDevice)

cudaFree(Device_V); // Liberamos memoria en GPU.
free(Host_V); // Liberamos memoria en CPU.
```
### Modelo pinned
- Reserva en memoria CPU que es accesible por la GPU.
- No requiere una transferencia de datos previa a los cálculos.
```C
// Declaramos variables necesarias.
double *Host_V, *Device_V;

// Creamos el espacio de memoria pinned en la memoria principal (CPU)
cudaHostAlloc((void**) &Host_V, size * sizeof(double), cudaHostAllocMapped);

// Obtenemos el puntero a la memoria pinned para usar en la GPU.
cudaHostGetDevicePointer( (void**) &Device_V, (void*) Host_V, 0)

// ...

cudaFreeHost(Host_V); // Liberamos memoria.
// No hace falta liberar Device_V ya que solo es un puntero.
```
### Modelo shared
- Reserva en memoria en un espacio compartido entre CPU y GPU (No sabemos donde)
- Parecida a pinned pero con mejor rendimiento. No todas las arquitecturas la soportan.
```C
// Declaramos variables necesarias.
double *V;

// Reservamos memoria en el espacio compartido.
cudaMallocManaged(&V, size * sizeof(double));

// ...

cudaFree(V); // Liberamos memoria.
```
---
## CUDA - Otras memorias
### Memorias constantes en GPU
```C++
__constant__ double Device_V[size]

int main() {
	// ...
	cudaMemcpyToSymbol(Device_V, Host_V, sizeof(Device_V))
	// ...
}
```
### *Shared memory* en la GPU
- Memoria de la GPU más rápida que la global pero con menos capacidad.
- Accesible por todos los hilos del mismo bloque
[Using Shared Memory in CUDA C/C++ | NVIDIA Technical Blog](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)
#### Estática (En compilación)
```C++
__global__ void kernel()
{
__shared__ int i;
__shared__ double V[100]; // No puede ser una variable
//...
}

kernel<<<NB, TPB>>>();
```
#### Dinámica (En runtime)
```C++
__globla__ void kernel()
{
extern __shared__ double V[];
//...
}

kernel<<<NB, TPB, size>>>();
```
#### Conflictos de banco
- La *shared memory* esta dividida en bancos (*banks*)
- **Broadcast**
	- Se produce cuando varios hilos acceden a la misma dirección del mismo banco.
	- Supone una sola transferencia. Muy rápido.
-  **Multicast**
	- Se produce cuando varios hilos acceden a direcciones en distintos bancos.
	- Supone una sola transferencia en nuevas arquitecturas. Muy rápido.
- **Conflicto de banco (Bank conflict)**
	- Se produce cuando varios hilos acceden a distintas direcciones en el mismo banco.
	- Se serializa el acceso. Muy lento.
![](../assets/bank.svg)

[NVIDIA CUDA Tutorial 9: Bank Conflicts](https://www.youtube.com/watch?v=CZgM3DEBplE)

---
## Saltos divergentes
- En caso de tener un condicional dentro del mismo warp.
- Todos los warps van a la vez pero se deshabilitan las líneas que no tengan que ejecutar.
- Esto es una pérdida de rendimiento
```C++
if(cond) {
	A(); // 1 - Todos los hilos aqui pero solo algunos lo ejecutan
	B(); // 2
} else {
	P(); // 3 - Los que antes ejecutaban ahora ya no y al revés
	R(); // 4
}
```

---
## Coalescencia
- Varios hilos consecutivos acceden a direcciones de memoria consecutivas.
- El acceso es coalescente = un solo ciclo = muy rápido
- Los hilos dentro de un mismo warp se ordenan por su identificador.
```c++
threadId = (
	threadIdx.x
	+ blockDim.x * threadIdx.y
	+ blockDim.x * blockDim.y * threadIdx.z
)
```

![](../assets/Pasted%20image%2020241203200700.png)

---
## Warp level primitives
- Los warps pueden compartir memoria sin necesidad de acceder a memoria local.
```C++
#define MASK 0xffffffff
__inline__ __device__ double ReduceWarp(double val)
{
	for (unsigned int offset = 16; offset > 0; offset /= 2)
		val += __shfl_down_sync(MASK, val, offset, 32);
	return val;
}
```
---
## Streams
- Los streams son una cadena de kernels que se ejecutan en orden.
- Varios streams se ejecutan en paralelo
```C++
cudaStream_t stream1, stream2;
cudaStreamCreate(&stream1);
cudaStreamCreate(&stream2);
...
cudaMalloc (&dev1, size);
cudaMallocHost (&host1, size); //tiene que ser pinned
...
cudaMemcpyAsync(dev1, host1, size, dir, stream1);
kernel2<<<grid, block, 0, stream2>>>(…, dev2, …);
...
alguna_función_cpu()
...
cudaStreamDestroy(stream1);
cudaStreamDestroy(stream2);
```
---
## Reducción
```C++
__global__ void reduce(int *g_idata, int *g_odata, unsigned int n) {
	
	extern __shared__ int sdata[]; // Definimos memoria compartida
	
	unsigned int tid = threadIdx.x; // ID en bloque
	unsigned int i = blockIdx.x * blockDim.x * 2 + threadIdx.x;
	unsigned int gridSize = blockSize * 2 * gridDim.x;
	
	sdata[tid] = 0;
	
	while (i < n) {
		sdata[tid] += g_idata[i] + g_idata[i + blockSize];
		i += gridSize;
	} __syncthreads();
	
	if (blockSize >= 512) {
		if (tid < 256) sdata[tid] += sdata[tid + 256];
		__syncthreads();
	}
	
	if (blockSize >= 256) {
		if (tid < 128) sdata[tid] += sdata[tid + 128];
		__syncthreads();
	}
	
	if (blockSize >= 128) {
		if (tid < 64) sdata[tid] += sdata[tid + 64];
		__syncthreads();
	}
	
	if (tid < 32) {
		sdata[tid] += sdata[tid + 32];
		sdata[tid] += sdata[tid + 16];
		sdata[tid] += sdata[tid + 8];
		sdata[tid] += sdata[tid + 4];
		sdata[tid] += sdata[tid + 2];
		sdata[tid] += sdata[tid + 1];
	}
	
	// Escribiendo resultado a la memoria global
	if (tid == 0) g_odata[blockIdx.x] = sdata[0];
}
```
---