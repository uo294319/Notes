# Machine Learning (ML)
---
[Go Back](../README.md)

---
## Definition
> **Machine Learning (ML)** is a field of Artificial Intelligence (AI) that studies statistical algorithms that can learn from data to perform tasks without explicit instructions.
## ML Problems
- Supervised (classes)
	- Classification (discrete values)
	- Regression (continuous values)
	- Ranking (order)
- Unsupervised (No classes)
- Dimensionality reduction
## Mathematical definition
- Given a dataset $S = \{(x_1, y_1), ..., (x_n, y_n)\}$
	- $X$ is the input $x_i \in X$ 
	- $Y$ is the output (classes) $y_i \in Y$ 
- We try to find $h: X \to Y$
- Such that we minimise the loss $\Delta(h(X), Y)$ for all test sets $S'$.
	- Classification Accuracy: $\Delta(h(X), Y) = \frac{1}{n}\sum_{i=1}^n1_{h(x_i)\neq y_i}$
	- Regression MAE: $\Delta(h(X), Y) = \frac{1}{n}\sum_{i=1}^n|h(x_i)- y_i|$
	- Regression MSE: $\Delta(h(X), Y) = \frac{1}{n}\sum_{i=1}^n(h(x_i)- y_i)^2$
## Classifiers
- Zero R.
	- Predicts always the most frequent class in the dataset
	- Uses zero attribute values (rules)
- One R
	- Predict based on the value of 1 attribute. 1-level decision tree.
	- How?
		- For each attribute in $X$ finde the most frequent class.
		- Compute the error rate and choose the one with less.
- Instance based learning
	- Training instance represent the knowledge. Lazy learning.
	- For new instances it finds the one with closest resemblance them.
	- For numeric values we can use distance.
	- Example. k-Nearest Neighbor (kNN)
## Decision Trees
- Components
	- Nodes (questions or tests)
		- Nominal (is the value?)
		- Continuous (compare with threshold)
	- Leaves (prediction tags)
- How to build them?
	- Smallest tree
	- Heuristic. Choose purest nodes or with more information gain (popular)
- C4.5 algorithm
	- Concept of Entropy that measures information in bits
	- Expression: $info(E) = -\sum_{j=1}^k \frac{frec(C_j, E)}{|E|} \times \log_2 \frac{frec(C_j, E)}{|E|}$
	- Being $C$ the class (Example. yes or no) and $E$ the dataset.
	- For numeric attributes, evaluate $info$ for every split point
## Support Vector Machines (SVM)
- **Support Vector Classifier**
	- Uses scalar products for classification. $<x, x'> = ||x|| \times ||x'||\times \cos(x, x')$
		- $-1 \leq\quad<x, x'>\quad\leq1$
			- Same: $<x, x'> = 1$
			- Perpendicular: $<x, x'> = 0$
			- Opposite: $<x, x'> = -1$
		- Scalar product measures similarity.
	- Splits the space in 2 regions by a flat Affine Hyperplane
		- A hyperplane is a subspace of dimension $n - 1$
		- It is defined by a perpendicular vector called director vector ($w$)
		- Afine hyperplanes are translated to a point $x_0$
	- Optimization
		- We try to maximise the distance between points and the affine hyperplane.
		- Maximal Margin Classifiers (Hard margin) are sensitive to outlier points.
		- We can use a soft margin (Support Vector Classifier)
			- Allow miss-classifications and outliers
			- To handle non-linearly separable or noisy data.
- **Support Vector Machines**
	- Starting from data with dimension $N$.
	- We need to increase dimensions. For that we use kernels.
		- Polynomial kernel of degree $d$ (for degree $d = 2$)
			- We can raise to the norm $||x||^2$ to calculate in a dimension $N+1$.
			- Value of $d$ can be found with cross validation.
		- Radial Kernel
	- Then we use a SVC for classification.
## Training and Testing
- We must measure the models performance
- Split into train and test data.
	- Independent data never used for training.
	- Both train and test should be representative samples of the problem.
- Proper way. Three sets
	- Stage 1. Build basic structure with train set.
	- Stage 2. Parameter tuning with validation set.
	- Stage 3. Testing with test set.
- Once model evaluation is complete:
	- We can use all the data for the training.
	- The larger the training set the better the classifier.
	- The larger the test set the more accurate the error estimate.
- How to divide data?
	- Holdout estimation.
		- One third reserved for testing and rest for training.
		- Stratification. Each class representation is approx. equal in both subsets.
	- Repeated holdout.
		- Several iteration with randomly selected division in subsets.
		- Overall error rate is the average of all iteration's error rate.
	- Cross Validation
		- We divide the dataset in $N$ stratified parts (folds)
		- We use some folds for training and the others for testing.
		- We rotate the folds used $N$ times. Each $N-1$ times for training.
		- Finally, we use all the $N$ folds for training before deployment.
	- Leave-One-Out (LOO) Cross Validation
		- $N$ is equal to the number of training instances (rows)
		- Maximises training data. Only one is used for testing.
		- Disadvantage. Not stratified.
---