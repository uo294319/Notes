# Deep Learning & CNN
---
[Go Back](../README.md)

---
## Deep Learning 
### Objective
- Given a dataset $D = {(x, y)  : x \leadsto y}$
- Design the non-linear function $f : R^m \to R^n, f(x, \theta) = f_\theta(x)$
- Find $\theta$ so that minimises the mean of the loss function
	- $\theta \leftarrow \frac{1}{|D|}\sum_{(x, y)\in D}loss(y, f_\theta(x))$
### Non linearity
- Activation functions
	- ReLu. $g(z) = \max(0, z)$ (most used nowadays)
	- Hyperbolic tangent. (-1 and 1)
	- Sigmoid. (0 and 1) (used for classification problems)
- Several nested layers:
	- Hidden layer 1. $h_1 = g(W_1x+b_1)$
	- Hidden layer 2. $h_2 = g(W_2h_1+b_2)$
	- Output layer. $g(x) =  g(W_2\times g(W_1x+b_1)+b_2)$
- Optimisation needs derivatives. Gradient Descent.
- Universal Approximator Theorem
	- One hidden layer is enough to represent an approximation of any function.
	- But choose wider networks because:
		- It avoids exponential # neurons needed.
		- Better generalisation and avoids overfitting.
### Types
- Feedforward or Multi-Layer Perceptrons (MLP) are fully connected.
- Convolutional Neural Networks (CNN). For processing data in grid-like topology.
- Recurrent Neural Networks (RNN). For processing sequential data. Extend MLP.
### Learning Tasks
- Regression.
	- $Pr(y|x, \theta) = N( y, f(x, \theta), \sigma^2)$.
	- Probability distribution involving $f$
	- Minimize MSE $\arg\min_\theta \sum_{D}(y - f_\theta(x, \theta))^2$
- Classification
	- Search for sth proportional to $\exp(yf(x,\theta))$.
		- Not a probability distribution.
	- Options.
		- Softmax. 
			- Multiclass Classification. Not a probability distribution.
			- $Pr(y|x,\theta) = \frac{\exp(yf(x, \theta))}{1 + f(x, \theta)}$
		- Sigmoid
			- Particularisation of softmax for binary classification (0 and 1).
			- It is a probability distribution.
			- $\sigma(x) = \frac{1}{1+\exp(-x)}$ 
			- $Pr(y|x,\theta) = \sigma((2y-1)f(x,\theta))$
	- In both we minimize $-log\quad Pr(y|x, \theta)$
### Final Remarks
- It uses back-propagation to avoid recomputing repeated sub-expressions.
- **Regularization**. Modifications to reduce its generalisation error but not training one.
	- Norm Penalties.
		- Add an extra term to the loss function that penalizes large weights
		- Prevents over-fitting.
	- Early Stopping
		- Terminate the training once the validation set performance is better.
	- Dropout.
		- Create subnets during training by disabling some non-output neurons.
---
## Convolutional Neural Network (CNN)
### Definition
> Convolutional Neural Networks (CNNs) are a class of deep neural networks specialised in processing grid-like data like images but also text and audio.

- They excel at:
	- Automatic feature extraction
	- Reducing parameters
	- Pattern recognition.
- Results
	- Image recognition
	- Natural Language Processing (NLP)
	- Generative AI (Deepfakes)
## Filters
- A **filter** is a small matrix (kernel) used to detect patters in the image.
	- The kernel slides through the input matrix and performs element-wise multiplication.
	- The values of the kernel are called weights.
	- The feature map (result matrix) highlights the detected patterns.
- **Padding**
	- Adding extra values around the input matrix.
	- This ensures that the result of applying the kernel has the same size than the input matrix.
- **Stride**
	- Controls how much the filter shifts over the input matrix.
## Architecture
- **Input Layer**
	- Receives the raw data
	- Generally a 3D matrix (height x width x channels)
	- May perform normalisation to improve training network efficiency
- **Convolutional Layer**
	- Uses kernels with padding and stride to produce feature maps.
	- It has an activation function (typically Re-Lu) to introduce non-linearity.
- **Pooling Layer**
	- Reduces spatial dimensions of feature maps for better efficiency and prevent over-fitting.
	- Max pooling. Takes the maximum value of a predefined window. (Common)
	- Average pooling. Takes the average of a predefined window (less frequent)
- **Fully Connected Layer**
	- Dense network. Neurons are connected to every neuron in the previous layer
	- Output of previous layers is flattened (usually a 1D vector) before this layer.
	- Combines extracted features and performs the decision-making.
	- Output probabilities. Activation function is sigmoid (binary) or soft-max (multi-class)
## Training
1. **Forward propagation.**
	1. Training data as input and obtain predictions.
	2. Uses layer hyper-parameters. Filters, padding, stride, units.
2. **Loss Calculation.**
	1. Compares output with actual labels.
	2. Uses Loss function. Cross-entropy for classification and MSE for regression.
3. **Back-propagation & Optimisation.**
	1. Computes gradients using loss function
	2. Updates weights with Gradient Descent
	3. Uses the
		1. Training parameters (weights)
		2. Optimiser (SGD, Adam)
		3. Optimisation hyper-parameters (learning rate)
4. **Iterations & Convergence.**
	1. Repeat until loss is minimised and model performance stabilises.
	2. Uses training hyper-parameters. Batch size (# iteration before update), epochs (# dataset)
5. **Validation & Regularization.**
	1. Evaluates with validation set to prevent over-fitting.
6. **Evaluation.**
	1. Performance metrics (Accuracy, MSE, MAE...)
---